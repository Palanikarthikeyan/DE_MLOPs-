{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1d06e-7b79-4f04-96d4-3b711727246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Batch Processing\n",
    "====================\n",
    "Process finite data (complete dataset) - stored in DB/File - fetch the complete data do process at once.\n",
    "df = pd.read_csv('emp.csv') \n",
    "df.shape ->(10,5)\n",
    "df.shape ->(10,5)\n",
    " Vs\n",
    "2.Stream Processing\n",
    "===================\n",
    "Process infinite data (or) real-time data\n",
    "- read ->transform ->write data\n",
    "\n",
    "    +------------------------+\n",
    "    | Streaming Source <== socket , file, kafka etc\n",
    "    +------------------------+\n",
    "             |\n",
    "     | SparkStructred Streaming |\n",
    "     |  (Transformation)        |\n",
    "             |\n",
    "     +-----------------------+\n",
    "     |  Streaming Sink  ---> Console,HDFS,Delta Lake,Kafa etc.,\n",
    "     +-----------------------+\n",
    "\n",
    "\n",
    "    DataStream                      Unbound Table    \n",
    "    -----------                   ----------------------\n",
    "       --------------->             ...\n",
    "    -----------                   ----------------------- \n",
    "                                    ...\n",
    "                                  -------------------------\n",
    "                                     ...\n",
    "                                   -------------------------\n",
    "\n",
    "    ---------->| SparkStream | -->[][][][]  ----->| SparkCore | --> outputSinks\n",
    "                                  inputbatches          \n",
    "outputmode\n",
    "->append mode => only new rows - not using aggregate method\n",
    "->update mode => Incremental value / recente value  - aggregate\n",
    "->complete mode => Full value / - aggregate \n",
    "\n",
    "\n",
    "1st spark session object\n",
    "2nd spark_session_object.interface to inputMode ->df\n",
    "3rd df.do_Transformation ->results\n",
    "4th result ->Sinkto_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40d001-e7da-4831-b8e0-428e09ef5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema()) \n",
    "write_query = df.writeStream.format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d121d9c-8081-4bc1-9ffc-f6b266f31396",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = df.writeStream.format(\"console\").start() \n",
    "same as \n",
    "write_query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "### \n",
    "df.select(explode(split(..,' ')))\n",
    "\n",
    "select()-query/filter\n",
    "|\n",
    "+---------+\n",
    "| value   |\n",
    "+---------+\n",
    "| data1 data2 data1\n",
    "+-------------------+\n",
    "   |\n",
    "explode() =>column->row \n",
    "                    |\n",
    "                 [data1|data2|data1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92f4a8-8cfd-4519-ad94-8bc03678e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema())\n",
    "write_query = df.writeStream.format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30df4c-3084-4765-9155-c9fe48141218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema())\n",
    "###\n",
    "df_result = df.select(explode(split(\"value\",\" \")).alias(\"word\"))\n",
    "\n",
    "word_count = df_result.groupBy(\"word\").count()\n",
    "###\n",
    "#\n",
    "# write_query = word_count.writeStream.format(\"console\").start()\n",
    "# Error - default outputMode is append\n",
    "\n",
    "write_query = word_count.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ed611-0786-453c-91d3-1d8802220643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "print(df.isStreaming) # ->bool(True/False)\n",
    "print(df.printSchema())\n",
    "###\n",
    "df_result = df.select(explode(split(\"value\",\" \")).alias(\"word\"))\n",
    "\n",
    "word_count = df_result.groupBy(\"word\").count()\n",
    "###\n",
    "#\n",
    "# write_query = word_count.writeStream.format(\"console\").start()\n",
    "# Error - default outputMode is append\n",
    "\n",
    "write_query = word_count.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "write_query.awaitTermination() # keep on running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0ca0a-a062-40c3-8e9b-c76e4818220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.create source directory (ex: input_dir/\n",
    "                                    - file1.csv\n",
    "                                    - file2.csv\n",
    "                                    - ..\n",
    "                                    - ...\n",
    ".................................................................\n",
    "1st =>sparksession_object\n",
    "2nd =>define schema  <== from pyspark.sql.types import *\n",
    "          |\n",
    "         StructType(StructField[\"pname\",StringType(),True],\n",
    "         StructType(StructField[\"pname\",IntegerType(),True])\n",
    "        ======================================================\n",
    "                |->initialize schema object\n",
    "\n",
    "3rd => read stream from input_dir/ \n",
    "            |\n",
    "        spark.readStream.option(\"header\",\"true\").schema(<schema object>).csv(input_dir/) \n",
    "\n",
    "4th =>process\n",
    "|\n",
    "5th => write streaming to console \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb47e8-87bb-436b-a884-0f0f303e2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"filestream\").getOrCreate()\n",
    "\n",
    "schema_obj = StructType([StructField(\"pname\",StringType(),True),StructField(\"pid\",IntegerType(),True)])\n",
    "\n",
    "file_stream = spark.readStream.option(\"header\",\"true\").schema(schema_obj).csv(\"input_dir/\")\n",
    "\n",
    "r = file_stream.filter(file_stream.pid >100).groupBy(\"pname\").count()\n",
    "\n",
    "r.writeStream.outputMode(\"complete\").format(\"console\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe21f1-f637-4fdb-bd98-119c1b223844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"filestream\").getOrCreate()\n",
    "\n",
    "schema_obj = StructType([StructField(\"pname\",StringType(),True),StructField(\"pid\",IntegerType(),True)])\n",
    "|\n",
    "file_stream = spark.readStream.option(\"header\",\"true\").schema(schema_obj).csv(\"input_dir/\")\n",
    "   Vs\n",
    "json_stream = spark.readStream.schema(schema_obj).json(\"json_dir/\")\n",
    "json_stream.writeStream.outputMode(\"append\").format(\"console\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524fc41-6472-473b-b196-54d57764b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir json_dir\n",
    "cd json_dir\n",
    "file:p1.py\n",
    "----------------------------------------\n",
    "import json\n",
    "d={'pname':'pA','pid':101}\n",
    "with open('data1.json','w') as wobj:\n",
    "    json.dump(d,wobj)\n",
    "|\n",
    "python p1.py\n",
    "|\n",
    "ls\n",
    "data1.json <== \n",
    "=============\n",
    "--------------------------------------------------------------------------\n",
    "Stream from socket ->   ...   ->write to csv file\n",
    "        ----------    ======     ====================\n",
    "                        |->append Vs update\n",
    "\n",
    "input_dir/\n",
    "    |->data1.csv\n",
    "    |->data2.csv\n",
    "\n",
    "output_dir/\n",
    "    |->result1.csv\n",
    "        =========\n",
    "         |->keep Header line ->option(\"header\",\"true\")\n",
    "\n",
    "1st create output directory (mkdir output_dir)\n",
    "..\n",
    "..\n",
    "stream_df.writeStream.outputMode(\"update/append\").format(\"csv\").option(\"path\",\"output_dir/\")\n",
    ".option(\"header\",\"true\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026d843-9eef-4a7a-981a-ebc845766a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream from socket ->process->write to csv file\n",
    "# --------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\")\n",
    ".option(\"port\",1120)\n",
    ".load()\n",
    "\n",
    "df.writeStream.outputMode(\"append\").format(\"csv\").option(\"path\",\"output_dir/\")\n",
    ".option(\"checkpointLocation\",\"checkpoints/csv_stream_chpt\")\n",
    ".option(\"header\",\"true\")\n",
    ".start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20cf67-582c-4480-bcfb-7f742e0bf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hello Good Morining  ->embedding->   [vector]            ...............\n",
    "                           |->ML         |->vectorDB <-- llm -\n",
    "(English)       -----------------> French \n",
    "\n",
    "End User: Get list of sales emp records //plain text/english \n",
    "          |___________________________|\n",
    "                |\n",
    "                llm\n",
    "                |\n",
    "                select *from emp <similarity_search- sales dept ...>\n",
    "                                  =================================== // SQL\n",
    "                                      where edept = \"sales\"  // SQL\n",
    "                |\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a7cd4-ef8a-4f8b-92b2-a6c22db381ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream from socket ->process-> write to csv file\n",
    "#                       |                       |->update mode \n",
    "#                       |->do aggregate - word count \n",
    "# --------------------------------------------------------------\n",
    "# append mode - wont' support aggregate operation\n",
    "# ------\n",
    "# update and complete mode support aggregate operation\n",
    "# --------------------\n",
    "# complete mode won't support for file sink - supports console (or) memory\n",
    "# ========\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('demo1').master('local[*]').getOrCreate()\n",
    "\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",1120).load()\n",
    "\n",
    "df_result = df.select(explode(split(\"value\",\" \")).alias(\"word\"))\n",
    "\n",
    "wc = df_result.groupBy(\"word\").count()\n",
    "wc.writeStream.outputMode(\"update\").format(\"csv\").option(\"path\",\"output_dir/\")\n",
    ".option(\"checkpointLocation\",\"checkpoints/csv_stream_chpt\")\n",
    ".option(\"header\",\"true\").start().awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c96700-e495-4f41-985b-31e7e604d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apache Flink \n",
    "------------------\n",
    "Flink - real-time data processing framework\n",
    "                  ===============\n",
    "                    |->process streaming data           Structured Stream\n",
    "API (or) Libs - Flink ML ; Table                Vs     DataFrame (or) DataSet\n",
    "-----------------------------------------------\n",
    "  Batch Process    Stream Process\n",
    "================================================\n",
    "    (Kernel) - Runtime Stream \n",
    "----------------------------------------------\n",
    "    Deployment - local ; cluster\n",
    "----------------------------------\n",
    " Storage - HDFS;DB;..\n",
    "--------------------------------------------\n",
    "\n",
    "Realtime Analytics\n",
    "in E-Commerce platform \n",
    "[]->    Apache kafka + Flink + ML+ElastricSearch -->Grafana (Visualization)\n",
    "\n",
    "(Source) -->Flink        -------->[Sink]\n",
    "              -Operators\n",
    "               map,filter,flatMap,keyby.\n",
    "    Operators like builtin function\n",
    "\n",
    "\n",
    "Appln\n",
    " <------>DB\n",
    " ------>GeneratesLog (events)\n",
    "                 |---------------------+Flink+Elaststic(or)Loki-->Grafana\n",
    "\n",
    "pip install apache-flink\n",
    "|\n",
    "pyflink/  <== from pyflink.datastream import StreamExecutionEnvironment\n",
    "    |\n",
    "    Set up -env   env=StreamExecutionEnvironment.get_environment()\n",
    "    |    \n",
    "    read   env.socket_text_stream(\"node\",<port>) ->data_stream\n",
    "    |\n",
    "    transformation    results=data_stream.flat_map(...)\n",
    "    |\n",
    "    sinks              results.add_sink(sink_obj)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffb107-c89f-45bd-8b38-52d05e889a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.common.typeinfo import Types\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "data = [\"hello data\",\"hello flink\",\"flink data streaming\"]\n",
    "text_stream = env.from_collection(data,type_info=Types.STRING())\n",
    "\n",
    "wc = (text_stream.flat_map(lambda line:[(w,1) for w in line.split()],output_type=Types.TUPLE([Types.STRING(),Types.INT()])).key_by(lambda x:x[0]).sum(1))\n",
    "wc.print()\n",
    "env.execute(\"simple wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779eaf7-6c2d-4a62-8a42-1a495c16f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream.connectors import StreamingFileSink\n",
    "\n",
    "#from pyflink.datastream.formats import CsvEncoder - Version 1.6 used\n",
    "\n",
    "from pyflink.common.serialization import Encoder\n",
    "import os\n",
    "\n",
    "\n",
    "class CsvEncoder(Encoder):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "    def encoder(self,value,stream):\n",
    "        line = \",\".join(map(str,value))+\"\\n\"\n",
    "        stream.write(line.encode(\"utf-8\"))\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "##################################################\n",
    "\n",
    "data = [\"hello stream\",\"hello flink\",\"flink streaming\"]\n",
    "text_stream = env.from_collection(data,type_info=Types.STRING())\n",
    "###################################################\n",
    "\n",
    "wc = (text_stream.flat_map(lambda a:[(w,1) for w in a.split()],output_type=Types.TUPLE([Types.STRING(),Types.INT()])).key_by(lambda x:x[0]).sum(1))\n",
    "######################################################\n",
    "var=\"output_csv_dir\"\n",
    "os.makedirs(var,exist_ok=True) # create output directory\n",
    "\n",
    "csv_sink = StreamingFileSink.for_row_format(var,CsvEncoder()).build()\n",
    "\n",
    "wc.add_sink(csv_sink) # Write results to csv\n",
    "\n",
    "#######################################################\n",
    "env.execute(\"Socket Stream to csvfile\")\n",
    "# submits the job to Flink runtime and starts execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4019f3-e738-486c-8b37-cfb58f996c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linux -> crontab - cron  ->crond //os - jobscheduling\n",
    "|\n",
    "Apache airflow\n",
    "=================\n",
    "|->Open-Source tool - data pipeline //code \n",
    "\n",
    "DAG\n",
    "---\n",
    "download_webcontent  <== t1\n",
    "extract data  <== t2\n",
    "|\n",
    "DataFrame <== t3\n",
    "|\n",
    "Insert to DB <--t4\n",
    "\n",
    "1. scheduler\n",
    "2. executor - task\n",
    "3. webUI\n",
    "4. Operator - predefined templates/scripts\n",
    "\n",
    "On Linux: python p1.py <==\n",
    "     |\n",
    "    vi p1.sh \n",
    "    python p1.py <==\n",
    "    :wq\n",
    "    chmod +x p1.sh\n",
    "    ./p1.sh <== running shellscript -- python code executed by bash\n",
    "\n",
    " 5. scheduler\n",
    "      - job1\n",
    "      - job2\n",
    "      - job3 \n",
    "        ..//queue \n",
    " 6. worker - instance\n",
    " 7. metadata \n",
    "     -- DAG run,status,log ...//\n",
    "\n",
    "airflow/\n",
    "     |->logs/\n",
    "     |->data/\n",
    "     |->dags/ <==\n",
    "          |  \n",
    "          |<-- job_schedule_task_in_python_code_Style //dagscript.py\n",
    "|\n",
    "After schedule this task\n",
    "|\n",
    "Start Webserver and Scheduler\n",
    " webserver => airflow webserver --port <portNumber> \n",
    " scheduler => airflow scheduler\n",
    "|\n",
    "Go to broswer => http://localhost:<port>{enter}\n",
    "\n",
    "        Login :  _____ <== airflow login\n",
    "        password: ____ <== airflow password\n",
    "    |\n",
    "  [DAG]\n",
    "    |-> dagName/id \n",
    "-------------------------------------------------------------------\n",
    "DAG_Script_template/format\n",
    "|\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "|\n",
    "ContextSwitch => with DAG(<params>) as dag_obj:\n",
    "\n",
    "<params>\n",
    "   |->dag_id=<userdefined dagName>\n",
    "      start_date=datetime(start up date) # YYYY,MM,DD \n",
    "      schedule_interval='@daily' 5minutes '@weekly' \n",
    "      catchup=<bool>\n",
    "               |->True - 2025,10,15 - scheduled date\n",
    "                                 |//pending jobs - run the pending jobs\n",
    "                       started on 17th oct\n",
    "\n",
    "     PythonOperator(task_id=<>,python_callable=<functionName>)\n",
    "     BashOperator()\n",
    "\n",
    "         job\n",
    "    +-----------+\n",
    "    |           |\n",
    "    +-----------+\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20ed1f-ff22-43f5-8e93-20134f5b93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python3 -m venv airflow_project\n",
    "#source airflow_project/bin/activate\n",
    "(airflow_project) #\n",
    "(airflow_project) #export AIRFLOW_VERSION=2.10.1\n",
    "(airflow_project) #export PYTHON_VERSION=\"$(python --version|cut -d\" \" -f 2|cut -d \".\" -f 1,2)\"\n",
    "(airflow_project) #export CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "(airflow_project) #\n",
    "(airflow_project) #pip install \"apache-airflow == ${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
    "(airflow_project) #airflow version\n",
    "2.10.1\n",
    "(airflow_project) #airflow db init\n",
    "(airflow_project) # To create airflow login and password\n",
    "airflow users create --username admin --firstname student --lastname user --role admin --email test@example.com --password admin\n",
    "|\n",
    "| To start webserver\n",
    "(airflow_project) #airflow webserver -p 8080 \n",
    "\n",
    "open another terminal => Activate env => \n",
    "                Start scheduler => airflow_project) student@paka:~/airflow-Demo$ airflow scheduler\n",
    "\n",
    "   |\n",
    "open a webbroswer => localhost:8080 {Enter}\n",
    "\n",
    "                    Login: admin\n",
    "                    Password: admin\n",
    "                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5def3-7d93-4acb-b52f-2491940c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(airflow_project) student@paka:~$ ls airflow\n",
    "airflow-webserver.pid  airflow.cfg  airflow.db  logs  webserver_config.py\n",
    "                            ------------------------------//there is no dags directory\n",
    "(airflow_project) student@paka:~$ mkdir -p ~/airflow/dags <== Create new dags directory\n",
    "(airflow_project) student@paka:~$ ls airflow\n",
    "airflow-webserver.pid  airflow.cfg  airflow.db  dags  logs  webserver_config.py\n",
    "(airflow_project) student@paka:~$              ------\n",
    "|\n",
    "Copy our dagscript.py file to ~/airflow/dags/ \n",
    "------------------------------------------------\n",
    "|\n",
    "Restart webserver and scheduler\n",
    "|\n",
    "open broswer ->127.0.0.1:8080 =>login: __  password:__\n",
    "========================================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
